{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepLearning_10-monkey.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlDNSgQ0oVcV"
      },
      "source": [
        "### Let's create our new model using an image size of 64 x 64\r\n",
        "from keras.applications import VGG16\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0hSWbIec5n"
      },
      "source": [
        "'''img_rows = 224\r\n",
        "img_cols = 224 '''\r\n",
        "img_rows = 64\r\n",
        "img_cols = 64\r\n",
        "\r\n",
        "train_path = '/content/drive/MyDrive/10_monkey/training'\r\n",
        "validation_path = '/content/drive/MyDrive/10_monkey/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9dM1YegHbd"
      },
      "source": [
        "# Re-loads the VGG16 model without the top or FC layers\r\n",
        "vgg16 = VGG16(weights = 'imagenet', include_top = False, input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4s6Pv0qR406"
      },
      "source": [
        "# Here we freeze the last 4 layers \r\n",
        "# Layers are set to trainable as True by default\r\n",
        "for layer in vgg16.layers:\r\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx8lh3AcR-k8"
      },
      "source": [
        "### Let's make a function that returns our FC Head\r\n",
        "def addTopModel(bottom_model, num_classes, D=256):\r\n",
        "    \"\"\"creates the top or head of the model that will be \r\n",
        "    placed ontop of the bottom layers\"\"\"\r\n",
        "    top_model = bottom_model.output\r\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\r\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\r\n",
        "    top_model = Dropout(0.3)(top_model)\r\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\r\n",
        "    return top_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_8EqkUySK6s",
        "outputId": "aa42e279-7758-43eb-c181-5209d2a74cb0"
      },
      "source": [
        "num_classes = 1\r\n",
        "\r\n",
        "FC_Head = addTopModel(vgg16, num_classes)\r\n",
        "\r\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 15,239,489\n",
            "Trainable params: 524,801\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuo7WlmsSYIQ"
      },
      "source": [
        "# use the image data generator to import the images from the datase\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "      rescale=1./255,\r\n",
        "      rotation_range=20,\r\n",
        "      width_shift_range=0.2,\r\n",
        "      height_shift_range=0.2,\r\n",
        "      horizontal_flip=True,\r\n",
        "      fill_mode='nearest')\r\n",
        " \r\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp0zXMfFStr2",
        "outputId": "75c3f13d-3b81-4aca-c83c-95c1c3a8f117"
      },
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\r\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/10_monkey/training',\r\n",
        "                                                 target_size = (64, 64),\r\n",
        "                                                 batch_size = 32,\r\n",
        "                                                 class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1098 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Ml4gVLS6Lt",
        "outputId": "22fdd9bc-03db-4c1b-d832-11e351203d71"
      },
      "source": [
        "validation_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/10_monkey/validation',\r\n",
        "                                                 target_size = (64,64),\r\n",
        "                                                 batch_size = 32,\r\n",
        "                                                 class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 278 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LsXwoTBTZrg",
        "outputId": "d6ad2922-a09d-45bf-cb3a-1c927b8ff7af"
      },
      "source": [
        "### Training using 64 x 64 image size is MUCH faster!\r\n",
        "\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n",
        "                   \r\n",
        "checkpoint = ModelCheckpoint(\"monkey10_vgg.h5\",\r\n",
        "                             monitor=\"val_loss\",\r\n",
        "                             mode=\"min\",\r\n",
        "                             save_best_only = True,\r\n",
        "                             verbose=1)\r\n",
        "\r\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \r\n",
        "                          min_delta = 0, \r\n",
        "                          patience = 5,\r\n",
        "                          verbose = 1,\r\n",
        "                          restore_best_weights = True)\r\n",
        "\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\r\n",
        "                              factor = 0.2,\r\n",
        "                              patience = 3,\r\n",
        "                              verbose = 1,\r\n",
        "                              min_delta = 0.00001)\r\n",
        "\r\n",
        "# we put our call backs into a callback list\r\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\r\n",
        "\r\n",
        "# Note we use a very small learning rate \r\n",
        "model.compile(loss = 'categorical_crossentropy',\r\n",
        "              optimizer = RMSprop(lr = 0.0001),\r\n",
        "              metrics = ['accuracy'])\r\n",
        "\r\n",
        "nb_train_samples = 1098\r\n",
        "nb_validation_samples = 278\r\n",
        "epochs = 25\r\n",
        "batch_size = 16\r\n",
        "\r\n",
        "history = model.fit(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\r\n",
        "    epochs = epochs,\r\n",
        "    callbacks = callbacks,\r\n",
        "    validation_data = validation_generator,\r\n",
        "    validation_steps = nb_validation_samples // batch_size)\r\n",
        "\r\n",
        "model.save(\"/content/drive/MyDrive/10_monkey/monkey10_vggV2.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "35/68 [==============>...............] - ETA: 1:16 - loss: 0.0000e+00 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1700 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 17 batches). You may need to use the repeat() function when building your dataset.\n",
            "68/68 [==============================] - 133s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00000, saving model to monkey10_vgg.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itqYQmIDURV5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}