{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_multiImage_classificatiafinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlE-74_8pziy",
        "outputId": "dd7f9963-db62-4695-ada8-b31433e71d7c"
      },
      "source": [
        "cd /content/drive/MyDrive/fruit360"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/fruit360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiyAQyaGq_7H"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soex3HLmtlcq",
        "outputId": "3e5408c3-efed-4673-c94e-d57f2c9b05ee"
      },
      "source": [
        "img_rows, img_cols = 100,100\n",
        "batch_size = 32\n",
        "#num_classes = 131\n",
        " \n",
        "train_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Training'\n",
        "validation_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Test'\n",
        " \n",
        "#Lets use some data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "                rescale=1./255,\n",
        "                rotation_range=30,\n",
        "                width_shift_range=0.3,\n",
        "                height_shift_range=0.3,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='nearest'\n",
        "                )\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                    train_data_dir,\n",
        "                    target_size = (img_rows,img_cols),\n",
        "                    batch_size=batch_size,\n",
        "                    class_mode = 'categorical',\n",
        "                    shuffle= True)\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "                    validation_data_dir,\n",
        "                    target_size = (img_rows,img_cols),\n",
        "                    batch_size=batch_size,\n",
        "                    class_mode = 'categorical',\n",
        "                    shuffle= False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 67692 images belonging to 131 classes.\n",
            "Found 22688 images belonging to 131 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRxBEi2St6-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25528fd0-8bd3-48f2-e3c8-15d6e999ede1"
      },
      "source": [
        "# Sequential layer banaune\n",
        "# padding - 'same' results in padding the input such that\n",
        "# the output has the same length as the original input\n",
        "num_classes = 131\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,(3,3), padding='same',input_shape=(img_rows, img_cols,3)))\n",
        " \n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(Conv2D(32,(3,3)))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "model.add(Conv2D(64,(3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        " \n",
        "classifier = Sequential()\n",
        "classifier.add(Dense(units=512, activation='relu'))\n",
        "classifier.add(Dense(units=256, activation='relu'))\n",
        "classifier.add(Dense(units=128, activation='relu'))\n",
        "classifier.add(Dense(units=81, activation='relu'))\n",
        "#classifier.add(Dense(units=32, activation='relu'))\n",
        "#classifier.add(Dense(units=16, activation='relu'))\n",
        "#classifier.add(Dense(units=8, activation='relu'))\n",
        "#classifier.add(Dense(units=4, activation='relu'))\n",
        " \n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 100, 100, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 100, 100, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 98, 98, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 98, 98, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 49, 49, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 49, 49, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 49, 49, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 49, 49, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 47, 47, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 47, 47, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 33856)             0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               17334784  \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 131)               67203     \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 131)               0         \n",
            "=================================================================\n",
            "Total params: 17,467,555\n",
            "Trainable params: 17,467,555\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSHVythrTbN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8a396e-5ae4-41bd-c840-811cc37bc6cc"
      },
      "source": [
        "# initiate RMSprop optimizer and configure smoe parameters\n",
        "#opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        " \n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/fruit360/fruits-360/fruit_V1.h5\", # yo model ley best weight lai save garcha\n",
        "                            monitor=\"val_loss\",\n",
        "                            mode=\"min\",\n",
        "                            save_best_only=True,\n",
        "                            verbose=1)\n",
        " \n",
        "earlystop = EarlyStopping(monitor = 'val_loss', #value being monitored for improvement\n",
        "                         min_delta = 0.001, # Abs value and is the main change required before we stop\n",
        "                         patience = 3, # no of epocs we wait before stopping\n",
        "                         verbose = 1,\n",
        "                         restore_best_weights=True) #keep the best weights once stopped\n",
        " \n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                           factor = 0.1,\n",
        "                           patience=3,\n",
        "                           verbose=1,\n",
        "                           min_delta=0.0001)\n",
        " \n",
        "# we put our callbacks into a callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        " \n",
        "# We use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "             optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        " \n",
        "nb_train_samples = 20000\n",
        "nb_validation_samples = 600\n",
        "epochs = 10\n",
        " \n",
        "history = model.fit(\n",
        "            train_generator,\n",
        "            steps_per_epoch = nb_train_samples//batch_size,\n",
        "            epochs = epochs,\n",
        "            callbacks = callbacks,\n",
        "            validation_data = validation_generator,\n",
        "            validation_steps = nb_validation_samples//batch_size)\n",
        "model.save(\"/content/drive/MyDrive/fruit360/fruits-360/fruit_V2.h5\") # yo model ley sabai lai save garcha"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 1060s 2s/step - loss: 4.2659 - accuracy: 0.0717 - val_loss: 2.5440 - val_accuracy: 0.1753\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.54400, saving model to /content/drive/MyDrive/fruit360/fruits-360/fruit_V1.h5\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1028s 2s/step - loss: 1.6800 - accuracy: 0.4814 - val_loss: 1.0788 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.54400 to 1.07883, saving model to /content/drive/MyDrive/fruit360/fruits-360/fruit_V1.h5\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1018s 2s/step - loss: 0.9786 - accuracy: 0.7042 - val_loss: 0.3202 - val_accuracy: 0.9306\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.07883 to 0.32017, saving model to /content/drive/MyDrive/fruit360/fruits-360/fruit_V1.h5\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1014s 2s/step - loss: 0.6958 - accuracy: 0.7816 - val_loss: 0.5355 - val_accuracy: 0.8212\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.32017\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1012s 2s/step - loss: 0.5542 - accuracy: 0.8346 - val_loss: 0.1795 - val_accuracy: 0.9340\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.32017 to 0.17953, saving model to /content/drive/MyDrive/fruit360/fruits-360/fruit_V1.h5\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1018s 2s/step - loss: 0.4801 - accuracy: 0.8565 - val_loss: 1.5085 - val_accuracy: 0.5365\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.17953\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1012s 2s/step - loss: 0.4663 - accuracy: 0.8624 - val_loss: 1.2189 - val_accuracy: 0.6510\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.17953\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 998s 2s/step - loss: 0.4529 - accuracy: 0.8742 - val_loss: 0.2918 - val_accuracy: 0.9115\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.17953\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_upR3zdOUtIp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}