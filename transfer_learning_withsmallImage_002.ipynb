{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "003_transfer_learning_withsmallImage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZtvu1sEEZ2C"
      },
      "source": [
        "## Can we speed this up?\r\n",
        "#### Let's try re-sizing the image to 64 x 64\r\n",
        "\r\n",
        "from keras.applications import VGG16\r\n",
        "\r\n",
        "# Setting the input size now to 64 x 64 pixel \r\n",
        "img_rows = 64\r\n",
        "img_cols = 64 \r\n",
        "\r\n",
        "# Re-loads the VGG16 model without the top or FC layers\r\n",
        "vgg16 = VGG16(weights = 'imagenet', \r\n",
        "                 include_top = False, \r\n",
        "                 input_shape = (img_rows, img_cols, 3))\r\n",
        "\r\n",
        "# Here we freeze the last 4 layers \r\n",
        "# Layers are set to trainable as True by default\r\n",
        "for layer in vgg16.layers:\r\n",
        "    layer.trainable = False\r\n",
        "    \r\n",
        "# Let's print our layers \r\n",
        "#for (i,layer) in enumerate(vgg16.layers):\r\n",
        "#    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFsKPIS5HEME"
      },
      "source": [
        "### Let's make a function that returns our FC Head\r\n",
        "def addTopModel(bottom_model, num_classes, D=256):\r\n",
        "    \"\"\"creates the top or head of the model that will be \r\n",
        "    placed ontop of the bottom layers\"\"\"\r\n",
        "    top_model = bottom_model.output\r\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\r\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\r\n",
        "    top_model = Dropout(0.3)(top_model)\r\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\r\n",
        "    return top_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZAqUG2zHGq7",
        "outputId": "7b9f756e-1cb3-4f35-cdb3-9c187ef2ab9e"
      },
      "source": [
        "### Let's add our FC Head back onto VGG\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "\r\n",
        "num_classes = 131\r\n",
        "\r\n",
        "FC_Head = addTopModel(vgg16, num_classes)\r\n",
        "\r\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 131)               33667     \n",
            "=================================================================\n",
            "Total params: 15,272,899\n",
            "Trainable params: 558,211\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZfpybeqFq9F",
        "outputId": "a36a785d-599e-4388-e6cf-16854b4a2d6c"
      },
      "source": [
        "### Let's create our new model using an image size of 64 x 64\r\n",
        "from keras.applications import VGG16\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Training'\r\n",
        "validation_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Test'\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "      rescale=1./255,\r\n",
        "      rotation_range=20,\r\n",
        "      width_shift_range=0.2,\r\n",
        "      height_shift_range=0.2,\r\n",
        "      horizontal_flip=True,\r\n",
        "      fill_mode='nearest')\r\n",
        " \r\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        " \r\n",
        "# Change the batchsize according to your system RAM\r\n",
        "train_batchsize = 16\r\n",
        "val_batchsize = 10\r\n",
        " \r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        train_data_dir,\r\n",
        "        target_size=(img_rows, img_cols),\r\n",
        "        batch_size=train_batchsize,\r\n",
        "        class_mode='categorical')\r\n",
        " \r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "        validation_data_dir,\r\n",
        "        target_size=(img_rows, img_cols),\r\n",
        "        batch_size=val_batchsize,\r\n",
        "        class_mode='categorical',\r\n",
        "        shuffle=False)\r\n",
        "\r\n",
        "# Re-loads the VGG16 model without the top or FC layers\r\n",
        "vgg16 = VGG16(weights = 'imagenet', \r\n",
        "                 include_top = False, \r\n",
        "                 input_shape = (img_rows, img_cols, 3))\r\n",
        "\r\n",
        "# Freeze layers\r\n",
        "for layer in vgg16.layers:\r\n",
        "    layer.trainable = False\r\n",
        "    \r\n",
        "# Number of classes in the Flowers-17 dataset\r\n",
        "#num_classes = 17\r\n",
        "\r\n",
        "FC_Head = addTopModel(vgg16, num_classes)\r\n",
        "\r\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 67692 images belonging to 131 classes.\n",
            "Found 22688 images belonging to 131 classes.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 131)               33667     \n",
            "=================================================================\n",
            "Total params: 15,272,899\n",
            "Trainable params: 558,211\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzHLYg3SGCL5",
        "outputId": "f8604a3b-c322-472d-a888-3a67228fb6a1"
      },
      "source": [
        "### Training using 64 x 64 image size is MUCH faster!\r\n",
        "\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n",
        "                   \r\n",
        "checkpoint = ModelCheckpoint(\"flowers_vgg_64.h5\",\r\n",
        "                             monitor=\"val_loss\",\r\n",
        "                             mode=\"min\",\r\n",
        "                             save_best_only = True,\r\n",
        "                             verbose=1)\r\n",
        "\r\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \r\n",
        "                          min_delta = 0, \r\n",
        "                          patience = 5,\r\n",
        "                          verbose = 1,\r\n",
        "                          restore_best_weights = True)\r\n",
        "\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\r\n",
        "                              factor = 0.2,\r\n",
        "                              patience = 3,\r\n",
        "                              verbose = 1,\r\n",
        "                              min_delta = 0.00001)\r\n",
        "\r\n",
        "# we put our call backs into a callback list\r\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\r\n",
        "\r\n",
        "# Note we use a very small learning rate \r\n",
        "model.compile(loss = 'categorical_crossentropy',\r\n",
        "              optimizer = RMSprop(lr = 0.0001),\r\n",
        "              metrics = ['accuracy'])\r\n",
        "\r\n",
        "nb_train_samples = 1120\r\n",
        "nb_validation_samples = 170\r\n",
        "epochs = 25\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "history = model.fit(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\r\n",
        "    epochs = epochs,\r\n",
        "    callbacks = callbacks,\r\n",
        "    validation_data = validation_generator,\r\n",
        "    validation_steps = nb_validation_samples // batch_size)\r\n",
        "\r\n",
        "model.save(\"/content/drive/MyDrive/fruit360/fruits_vggV2.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "35/35 [==============================] - 516s 15s/step - loss: 5.1753 - accuracy: 0.0102 - val_loss: 4.3236 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.32361, saving model to flowers_vgg_64.h5\n",
            "Epoch 2/25\n",
            "35/35 [==============================] - 493s 14s/step - loss: 4.9146 - accuracy: 0.0109 - val_loss: 4.5027 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 4.32361\n",
            "Epoch 3/25\n",
            "35/35 [==============================] - 458s 13s/step - loss: 4.8426 - accuracy: 0.0298 - val_loss: 4.6550 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 4.32361\n",
            "Epoch 4/25\n",
            "35/35 [==============================] - 462s 13s/step - loss: 4.8080 - accuracy: 0.0306 - val_loss: 4.6934 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 4.32361\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "Epoch 5/25\n",
            "35/35 [==============================] - 457s 13s/step - loss: 4.7525 - accuracy: 0.0402 - val_loss: 4.6710 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 4.32361\n",
            "Epoch 6/25\n",
            "35/35 [==============================] - 452s 13s/step - loss: 4.7466 - accuracy: 0.0338 - val_loss: 4.6448 - val_accuracy: 0.0000e+00\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 4.32361\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDThj_v0L6HS"
      },
      "source": [
        "# custom definition for confusion matrix\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "def plot_confusion_matrix(cm, classes,\r\n",
        "                          normalize=False, #if true all values in confusion matrix is between 0 and 1\r\n",
        "                          title='Confusion matrix',\r\n",
        "                          cmap=plt.cm.Blues):\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(len(classes))\r\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "    plt.yticks(tick_marks, classes)\r\n",
        "\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "        print(\"Normalized confusion matrix\")\r\n",
        "    else:\r\n",
        "        print('Confusion matrix, without normalization')\r\n",
        "\r\n",
        "    print(cm)\r\n",
        "    thresh = cm.max() / 2.\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        plt.text(j, i, cm[i, j],\r\n",
        "                 horizontalalignment=\"center\",\r\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label')\r\n",
        "\r\n",
        "#%% print classification report and plot confusion matrix\r\n",
        "\r\n",
        "target_names = ['barretts', 'bbps-0-1', 'bbps-2-3', 'cecum', 'dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis-a', 'esophagitis-b-d', 'impacted-stool', 'polyps', 'pylorus', 'retroflex-rectum', 'retroflex-stomach', 'ulcerative-colitis-grade-0-1', 'z-line'] \r\n",
        "\r\n",
        "# Compute confusion matrix\r\n",
        "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\r\n",
        "np.set_printoptions(precision=4)\r\n",
        "\r\n",
        "# Plot non-normalized confusion matrix\r\n",
        "plt.figure(figsize=(20,10), dpi=300)\r\n",
        "plot_confusion_matrix(cnf_matrix, classes=target_names)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IduIXfBoiMo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}