{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_image_classification_callbacks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pfNS_-62T7w",
        "outputId": "3b037e22-28ca-4c2b-e804-1658573292da"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvyyL0Wk_z5s"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJl3NFsdOwHb"
      },
      "source": [
        "model = Sequential()\r\n",
        "#step1- Convolution\r\n",
        "model.add(Conv2D(256,(3,3), input_shape=(64,64,3), activation = 'relu'))\r\n",
        "#step2-Maxpooling\r\n",
        "model.add(MaxPooling2D(pool_size= (2,2)))\r\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tARIUnpwdXQ7"
      },
      "source": [
        "#Adding second convolutional layer\r\n",
        "#step1- Convolution\r\n",
        "model.add(Conv2D(128,(3,3),activation='relu'))\r\n",
        "#step2 -Convolution\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.2)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQrYP7D4fH2D",
        "outputId": "82591d18-5239-4cab-a6bb-61d873217257"
      },
      "source": [
        "# Adding third convolutional layer\r\n",
        "#step1- Convolution\r\n",
        "#model.add(Conv2D(64,(3,3),activation='relu'))\r\n",
        "##step2 -Convolution\r\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.25)) \r\n",
        "\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(256))\r\n",
        "#model.add(activation='relu')\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 256)       7168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 29, 29, 128)       295040    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "=================================================================\n",
            "Total params: 6,790,784\n",
            "Trainable params: 6,790,784\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtUPtf5kjQnq"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\r\n",
        "                                   shear_range = 0.2,\r\n",
        "                                   width_shift_range = 0.3,\r\n",
        "                                   height_shift_range = 0.3,\r\n",
        "                                   zoom_range = 0.2,\r\n",
        "                                   horizontal_flip = True,\r\n",
        "                                   fill_mode = 'nearest' \r\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0oVARwJqLqA"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4fxDlJdqP4F",
        "outputId": "90ec1136-318f-4780-ee33-2c9ffc215bd8"
      },
      "source": [
        "batch_size = 32\r\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/dogvscat/archive/training_set/training_set',\r\n",
        "                                                 target_size=(64,64),\r\n",
        "                                                 batch_size = batch_size,\r\n",
        "                                                 class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8005 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Nfd18JyqyMf",
        "outputId": "d00cb33a-4edd-44dd-ecf1-506b35c7b320"
      },
      "source": [
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/dogvscat/archive/test_set/test_set',\r\n",
        "                                                 target_size=(64,64),\r\n",
        "                                                 batch_size = batch_size,\r\n",
        "                                                 class_mode = 'categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2023 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcxBZYqvq8yI"
      },
      "source": [
        "# full connection\r\n",
        "model.add(Dense(units=512,activation='relu'))\r\n",
        "model.add(Dense(units=256,activation='relu'))\r\n",
        "model.add(Dense(units = 128,activation='relu'))\r\n",
        "model.add(Dense(units = 64,activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(units = 2,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dxilcY3xz2V"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZMLm_TM159q",
        "outputId": "5b65dcd9-a730-4d3d-b1e6-8d83e5e016bc"
      },
      "source": [
        "history = model.fit(training_set,\r\n",
        "          steps_per_epoch = 8005//batch_size,\r\n",
        "          validation_data = test_set,\r\n",
        "          validation_steps = 2023//batch_size,\r\n",
        "          epochs = 5)\r\n",
        "model.save('/content/drive/MyDrive/dogvscat/cat_vs_dogV4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 4139s 17s/step - loss: 0.6960 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 406s 2s/step - loss: 0.6929 - accuracy: 0.5117 - val_loss: 0.6920 - val_accuracy: 0.5327\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 406s 2s/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 405s 2s/step - loss: 0.6934 - accuracy: 0.5046 - val_loss: 0.6931 - val_accuracy: 0.5064\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 405s 2s/step - loss: 0.6926 - accuracy: 0.5063 - val_loss: 0.6913 - val_accuracy: 0.5154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AGxjD3S17Dx"
      },
      "source": [
        "from keras.optimizers import RMSprop, SGD\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n",
        "\r\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5',\r\n",
        "                            monitor = 'val_loss',\r\n",
        "                            mode = 'min',\r\n",
        "                            save_best_only = True,\r\n",
        "                            verbose = 1)\r\n",
        "earlystop = EarlyStopping(monitor = 'val_loss',\r\n",
        "                          min_delta = 0.001,\r\n",
        "                          patience = 3,\r\n",
        "                          verbose = 1,\r\n",
        "                          restore_best_weights = True)\r\n",
        "\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\r\n",
        "                              factor = 0.1,\r\n",
        "                              patience = 3,\r\n",
        "                              verbose = 1,\r\n",
        "                              min_delta = 0.0001)\r\n",
        "\r\n",
        "callbacks = (earlystop, checkpoint, reduce_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAhgSmq4AbJs"
      },
      "source": [
        "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT1DrMmMXQAY",
        "outputId": "f28cf227-026d-4731-e188-15e8dbe07ba4"
      },
      "source": [
        "nb_train_samples = 8005\r\n",
        "nb_validation_samples = 2023\r\n",
        "epochs = 15\r\n",
        "history = model.fit(training_set,\r\n",
        "          steps_per_epoch = nb_train_samples//batch_size,\r\n",
        "          validation_data = test_set,\r\n",
        "          validation_steps = nb_validation_samples//batch_size,\r\n",
        "          callbacks = callbacks,\r\n",
        "          epochs = epochs)\r\n",
        "model.save('/content/drive/MyDrive/dogvscat/cat_vs_dogV6.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 6801s 27s/step - loss: 0.9709 - accuracy: 0.4994 - val_loss: 0.6916 - val_accuracy: 0.5258\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69159, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 352s 1s/step - loss: 0.6955 - accuracy: 0.5197 - val_loss: 0.6803 - val_accuracy: 0.5590\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69159 to 0.68025, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 353s 1s/step - loss: 0.7139 - accuracy: 0.5459 - val_loss: 0.6789 - val_accuracy: 0.5809\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.68025 to 0.67894, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 353s 1s/step - loss: 0.7092 - accuracy: 0.5661 - val_loss: 0.6594 - val_accuracy: 0.5962\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67894 to 0.65943, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 351s 1s/step - loss: 0.6899 - accuracy: 0.5766 - val_loss: 0.6815 - val_accuracy: 0.5427\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.65943\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 353s 1s/step - loss: 0.6773 - accuracy: 0.5829 - val_loss: 0.6517 - val_accuracy: 0.6508\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.65943 to 0.65174, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 352s 1s/step - loss: 0.6894 - accuracy: 0.5942 - val_loss: 0.6253 - val_accuracy: 0.6677\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.65174 to 0.62526, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 350s 1s/step - loss: 0.6701 - accuracy: 0.5969 - val_loss: 0.6317 - val_accuracy: 0.6295\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.62526\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 352s 1s/step - loss: 0.6684 - accuracy: 0.6212 - val_loss: 0.5963 - val_accuracy: 0.6984\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.62526 to 0.59631, saving model to /content/drive/MyDrive/dogvscat/cat_vs_dogv6.h5\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 350s 1s/step - loss: 0.6717 - accuracy: 0.6408 - val_loss: 0.8013 - val_accuracy: 0.5228\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.59631\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 352s 1s/step - loss: 0.6637 - accuracy: 0.6275 - val_loss: 0.5975 - val_accuracy: 0.6954\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.59631\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 352s 1s/step - loss: 0.6425 - accuracy: 0.6429 - val_loss: 0.6195 - val_accuracy: 0.6548\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.59631\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWcqfazLUcOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5995586c-ae50-418a-bbfb-647e2f9653f4"
      },
      "source": [
        "from keras.preprocessing import image\r\n",
        "from keras.models import load_model\r\n",
        "import numpy as np\r\n",
        "classifier = load_model('/content/drive/MyDrive/dogvscat/cat_vs_dogV6.h5')\r\n",
        "test_image = image.load_img('/content/drive/MyDrive/dogvscat/archive/test_set/test_set/dogs/dog.4008.jpg', target_size=(64, 64, 3))\r\n",
        "test_image = image.img_to_array(test_image)\r\n",
        "test_image = np.expand_dims(test_image,axis=0)\r\n",
        "result = classifier.predict(test_image)\r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUAHbWQ5QK2h"
      },
      "source": [
        "# Displaying our Confusion Matrix\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "#Confution Matrix and Classification Report\r\n",
        "Y_pred = classifier.predict_generator(test_set, nb_validation_samples // batch_size+1) # put classifier not model\r\n",
        "y_pred = np.argmax(Y_pred, axis=1)\r\n",
        "print('Confusion Matrix')\r\n",
        "print(confusion_matrix(validation_generator.classes, y_pred))\r\n",
        "print('Classification Report')\r\n",
        "target_names = list(class_labels.values())\r\n",
        "print(classification_report(test_set.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3BF41LQlrt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}