{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "002_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCO87nxbELR7"
      },
      "source": [
        "# Making a Flower Classifier with VGG16\r\n",
        "\r\n",
        "### Loading the VGG16 Model\r\n",
        "from keras.applications import VGG16\r\n",
        "\r\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\r\n",
        "img_rows = 224\r\n",
        "img_cols = 224 \r\n",
        "\r\n",
        "#Loads the VGG16 model \r\n",
        "vgg16 = VGG16(weights = 'imagenet', include_top = False, input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u73shg8YEi4Z",
        "outputId": "388a7d42-299e-44e4-a095-86c82d0e676e"
      },
      "source": [
        "### Inpsecting each layer\r\n",
        "# Let's print our layers \r\n",
        "for (i,layer) in enumerate(vgg16.layers):\r\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer True\n",
            "1 Conv2D True\n",
            "2 Conv2D True\n",
            "3 MaxPooling2D True\n",
            "4 Conv2D True\n",
            "5 Conv2D True\n",
            "6 MaxPooling2D True\n",
            "7 Conv2D True\n",
            "8 Conv2D True\n",
            "9 Conv2D True\n",
            "10 MaxPooling2D True\n",
            "11 Conv2D True\n",
            "12 Conv2D True\n",
            "13 Conv2D True\n",
            "14 MaxPooling2D True\n",
            "15 Conv2D True\n",
            "16 Conv2D True\n",
            "17 Conv2D True\n",
            "18 MaxPooling2D True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPPT-VK1JhKg",
        "outputId": "19a21310-11c2-4826-c8ea-db31979ba954"
      },
      "source": [
        "### Let's freeze all layers except the top 4 \r\n",
        "from keras.applications import VGG16\r\n",
        "\r\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\r\n",
        "img_rows = 224\r\n",
        "img_cols = 224 \r\n",
        "\r\n",
        "# Re-loads the VGG16 model without the top or FC layers\r\n",
        "vgg16 = VGG16(weights = 'imagenet', \r\n",
        "                 include_top = False, \r\n",
        "                 input_shape = (img_rows, img_cols, 3))\r\n",
        "\r\n",
        "# Here we freeze the last 4 layers \r\n",
        "# Layers are set to trainable as True by default\r\n",
        "for layer in vgg16.layers: # vgg ko layers haru already trained bhaisakeko huncha teslai feri train garna pardaina tesaile train = False garne\r\n",
        "    layer.trainable = False # tesaile we make sure each layer in vgg is not trained again\r\n",
        "    \r\n",
        "# Let's print our layers \r\n",
        "for (i,layer) in enumerate(vgg16.layers):\r\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG8o6dZzJ0sk"
      },
      "source": [
        "### Let's make a function that returns our FC Head\r\n",
        "def addTopModel(bottom_model, num_classes, D=256):\r\n",
        "    \"\"\"creates the top or head of the model that will be \r\n",
        "    placed ontop of the bottom layers\"\"\"\r\n",
        "    top_model = bottom_model.output\r\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\r\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\r\n",
        "    top_model = Dropout(0.3)(top_model)\r\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\r\n",
        "    return top_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw09XA2mKOuM",
        "outputId": "e526011b-73c4-49fe-a2ab-b3a7850aacf1"
      },
      "source": [
        "### Let's add our FC Head back onto VGG\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "\r\n",
        "num_classes = 131\r\n",
        "\r\n",
        "FC_Head = addTopModel(vgg16, num_classes)\r\n",
        "\r\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 131)               33667     \n",
            "=================================================================\n",
            "Total params: 21,171,139\n",
            "Trainable params: 6,456,451\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXir06-YKVe0",
        "outputId": "20a96778-5d02-4dff-d977-31da3debcc22"
      },
      "source": [
        "### Loading our Flowers Dataset\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Training'\r\n",
        "validation_data_dir = '/content/drive/MyDrive/fruit360/fruits-360/Test'\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "      rescale=1./255,\r\n",
        "      rotation_range=20,\r\n",
        "      width_shift_range=0.2,\r\n",
        "      height_shift_range=0.2,\r\n",
        "      horizontal_flip=True,\r\n",
        "      fill_mode='nearest')\r\n",
        " \r\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        " \r\n",
        "# Change the batchsize according to your system RAM\r\n",
        "#train_batchsize = 256\r\n",
        "#val_batchsize = 128\r\n",
        " \r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        train_data_dir,\r\n",
        "        target_size=(img_rows, img_cols),\r\n",
        "        #batch_size=train_batchsize,\r\n",
        "        class_mode='categorical')\r\n",
        " \r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "        validation_data_dir,\r\n",
        "        target_size=(img_rows, img_cols),\r\n",
        "        #batch_size=val_batchsize,\r\n",
        "        class_mode='categorical',\r\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 67692 images belonging to 131 classes.\n",
            "Found 22688 images belonging to 131 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukl61YbKNeXZ",
        "outputId": "22ef94a9-8e31-4168-85c5-0084bcfba713"
      },
      "source": [
        "### Training our top layers\r\n",
        "from keras.optimizers import RMSprop\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\r\n",
        "                   \r\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/fruit360/fruits_vgg.h5\",\r\n",
        "                             monitor=\"val_loss\",\r\n",
        "                             mode=\"min\",\r\n",
        "                             save_best_only = True,\r\n",
        "                             verbose=1)\r\n",
        "\r\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \r\n",
        "                          min_delta = 0, \r\n",
        "                          patience = 3,\r\n",
        "                          verbose = 1,\r\n",
        "                          restore_best_weights = True)\r\n",
        "\r\n",
        "# we put our call backs into a callback list\r\n",
        "callbacks = [earlystop, checkpoint]\r\n",
        "# Note we use a very small learning rate \r\n",
        "model.compile(loss = 'categorical_crossentropy',\r\n",
        "              optimizer = RMSprop(lr = 0.001),\r\n",
        "              metrics = ['accuracy'])\r\n",
        "\r\n",
        "nb_train_samples = 67692\r\n",
        "nb_validation_samples = 22688\r\n",
        "epochs = 3\r\n",
        "batch_size = 512\r\n",
        "\r\n",
        "history = model.fit_generator(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\r\n",
        "    epochs = epochs,\r\n",
        "    callbacks = callbacks,\r\n",
        "    validation_data = validation_generator,\r\n",
        "    validation_steps = nb_validation_samples // batch_size)\r\n",
        "\r\n",
        "model.save(\"/content/drive/MyDrive/fruit360/fruits_vgg.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "132/132 [==============================] - 2869s 22s/step - loss: 5.9873 - accuracy: 0.0136 - val_loss: 5.2563 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.25634, saving model to /content/drive/MyDrive/fruit360/fruits_vgg.h5\n",
            "Epoch 2/3\n",
            "132/132 [==============================] - 2871s 22s/step - loss: 4.6240 - accuracy: 0.0450 - val_loss: 4.8426 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.25634 to 4.84263, saving model to /content/drive/MyDrive/fruit360/fruits_vgg.h5\n",
            "Epoch 3/3\n",
            "132/132 [==============================] - 2864s 22s/step - loss: 4.2986 - accuracy: 0.0745 - val_loss: 4.9003 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 4.84263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXVinXuSs6nU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}